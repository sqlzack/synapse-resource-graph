{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Set Variables and Import Modules for use throughout Notebook"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "resourceGraphURL = 'https://management.azure.com/providers/Microsoft.ResourceGraph/resources?api-version=2020-04-01-preview'\r\n",
        "\r\n",
        "resourceGraphQuery = \"\"\"\r\n",
        "securityresources\r\n",
        "| where type == 'microsoft.security/assessments'\r\n",
        "| where properties.metadata.severity == 'High'\r\n",
        "| where properties.metadata.implementationEffort == 'Low'\r\n",
        "| summarize ThreatCount=count() by Threats=tostring(properties.metadata.threats) \t\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "saveDestination = \"abfss://synapse@zmdeploytestsa20230808.dfs.core.windows.net/rawjson/resourcesapifromspark/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkXXS",
              "session_id": "3",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-08-14T16:32:06.6323428Z",
              "session_start_time": null,
              "execution_start_time": "2023-08-14T16:32:06.7730661Z",
              "execution_finish_time": "2023-08-14T16:32:06.9437312Z",
              "spark_jobs": null,
              "parent_msg_id": "5a8017f6-70f4-4af1-899a-073386067045"
            },
            "text/plain": "StatementMeta(sparkXXS, 3, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the POST call to the REST API "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def callAPI(skipToken = \"\"):\r\n",
        "    token = mssparkutils.credentials.getToken('AzureManagement')\r\n",
        "    auth_headers = {'Authorization': 'Bearer ' + token}\r\n",
        "\r\n",
        "    if skipToken != \"\":\r\n",
        "        data = {\"query\": resourceGraphQuery,\"options\":{\"$skipToken\": skipToken} }\r\n",
        "    else:\r\n",
        "        data = {\"query\": resourceGraphQuery}\r\n",
        "\r\n",
        "    resourceGraphResponse = requests.post(resourceGraphURL,headers=auth_headers,json=data)\r\n",
        "\r\n",
        "    responseJSON = resourceGraphResponse.text\r\n",
        "\r\n",
        "    return responseJSON"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkXXS",
              "session_id": "3",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-08-14T16:32:06.7096501Z",
              "session_start_time": null,
              "execution_start_time": "2023-08-14T16:32:07.1176348Z",
              "execution_finish_time": "2023-08-14T16:32:07.3482819Z",
              "spark_jobs": null,
              "parent_msg_id": "647eb503-d07e-474b-8176-bb95f20eace7"
            },
            "text/plain": "StatementMeta(sparkXXS, 3, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Python Function to parse API Response into a usable Data Frame"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parseResponse(responseJSONText):\r\n",
        "    # Parallelize JSON string using Spark Context into a Pyspark Data Frame\r\n",
        "    responseDF=spark.read.json(sc.parallelize([responseJSONText]))\r\n",
        "\r\n",
        "    # Create a temp view so data can be transformed using SQL\r\n",
        "    responseDF.createOrReplaceTempView(\"v_tempJSON\")\r\n",
        "\r\n",
        "    # SQL Query used to parse the responseDF\r\n",
        "    sqlQuery = \"\"\"\r\n",
        "    WITH baseExplode AS\r\n",
        "    (\r\n",
        "    SELECT explode(data.rows) rowData\r\n",
        "    FROM v_tempJSON\r\n",
        "    )\r\n",
        "    SELECT  rowData[0] ThreatName\r\n",
        "            ,rowData[1] ThreatCount\r\n",
        "    FROM baseExplode\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Parse response using Spark SQL\r\n",
        "    dfParsedResponse = spark.sql(sqlQuery)\r\n",
        "\r\n",
        "    return dfParsedResponse"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkXXS",
              "session_id": "3",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-08-14T16:32:06.8100636Z",
              "session_start_time": null,
              "execution_start_time": "2023-08-14T16:32:07.5019471Z",
              "execution_finish_time": "2023-08-14T16:32:07.6624256Z",
              "spark_jobs": null,
              "parent_msg_id": "91deddc9-f0d8-4b81-af64-485d6347ccc7"
            },
            "text/plain": "StatementMeta(sparkXXS, 3, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run initial process to extract data from REST"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Response Text from API to a variable\r\n",
        "responseText = callAPI()\r\n",
        "\r\n",
        "# Parse Columns in Response Text to a Data Frame\r\n",
        "dfParsedResponse = parseResponse(responseText)\r\n",
        "\r\n",
        "# Write the Data Frame to Parquet files on storage.\r\n",
        "dfParsedResponse.write.mode(\"overwrite\").parquet(saveDestination)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkXXS",
              "session_id": "3",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-08-14T16:32:06.8995524Z",
              "session_start_time": null,
              "execution_start_time": "2023-08-14T16:32:07.8132651Z",
              "execution_finish_time": "2023-08-14T16:32:45.7365327Z",
              "spark_jobs": null,
              "parent_msg_id": "5662e245-20d1-4373-8215-1e05f93b0d77"
            },
            "text/plain": "StatementMeta(sparkXXS, 3, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check if API returned a Skip Token and run until it does not"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format Response Text to JSON\r\n",
        "responseJSON = json.loads(responseText)\r\n",
        "\r\n",
        "# Evaluate whether response contains a Skip Token\r\n",
        "while \"$skipToken\" in responseJSON:\r\n",
        "    # Set Skip Token Variable\r\n",
        "    skiptoken = responseJSON[\"$skipToken\"]\r\n",
        "\r\n",
        "    # Call the API using the Skip Token option\r\n",
        "    responseText = callAPI(skiptoken)\r\n",
        "\r\n",
        "    # Set a new response variable to be evaluated in while loop\r\n",
        "    responseJSON = json.loads(responseText)\r\n",
        "\r\n",
        "    # Create a new data frame from new API Response\r\n",
        "    dfParsedResponse = parseResponse(responseText)\r\n",
        "\r\n",
        "    # Append Parquet table created in previous steps.\r\n",
        "    dfParsedResponse.write.mode(\"append\").parquet(saveDestination)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkXXS",
              "session_id": "3",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-08-14T16:32:07.0336558Z",
              "session_start_time": null,
              "execution_start_time": "2023-08-14T16:32:45.9243107Z",
              "execution_finish_time": "2023-08-14T16:32:46.1023365Z",
              "spark_jobs": null,
              "parent_msg_id": "c37b0059-78d4-4f33-88ca-34412036756c"
            },
            "text/plain": "StatementMeta(sparkXXS, 3, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __OPTIONAL__: Create Table in Lake Database to make for easy Power BI integration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create query variable (I like to format as multi-line, hence the triple quotes)\r\n",
        "createTableStatement = f\"\"\"\r\n",
        "CREATE TABLE IF NOT EXISTS resourceGraphLakeDb.threatSummary\r\n",
        "USING PARQUET\r\n",
        "LOCATION '{saveDestination}'\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# Create database and table if they do not exist.\r\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS resourceGraphLakeDb\")\r\n",
        "spark.sql(createTableStatement)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkXXS",
              "session_id": "3",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-08-14T16:32:07.1095382Z",
              "session_start_time": null,
              "execution_start_time": "2023-08-14T16:32:46.2560864Z",
              "execution_finish_time": "2023-08-14T16:33:05.2579187Z",
              "spark_jobs": null,
              "parent_msg_id": "c2e7d1a1-4e85-4f4b-a508-4d4d9bb3590f"
            },
            "text/plain": "StatementMeta(sparkXXS, 3, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "DataFrame[]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}